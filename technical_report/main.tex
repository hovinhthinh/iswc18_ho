\documentclass[a4paper]{llncs}

%\documentclass{llncs}
%
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{makeidx}  % allows for indexgeneration
%
\usepackage{epsfig} 
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{color}
\usepackage{multirow}
\usepackage{subfig}
\usepackage{hyperref}
\usepackage[para]{footmisc}
\hypersetup{
    colorlinks=true,
    %linkcolor=blue,
%    filecolor=magenta,      
    urlcolor=blue,
    citecolor=blue,
    linkcolor=blue,
}
\usepackage{tikz}

\usetikzlibrary{positioning}
\usetikzlibrary{arrows}
\usetikzlibrary{shapes.geometric, arrows}

\usepackage[inline]{enumitem}
\input{draft_tools}
\input{commands}


\usepackage{llncs-space}

\newcommand{\G}{\ensuremath{\mathcal{G}}\xspace}
\newcommand{\X}{\ensuremath{\mathcal{X}}\xspace}
\newcommand{\C}{\ensuremath{\mathcal{C}}\xspace}
\newcommand{\R}{\ensuremath{\mathcal{R}}\xspace}
\newcommand{\PG}{\ensuremath{\mathcal{P}}\xspace}
\newcommand{\PW}{\ensuremath{\mathsf{PW}}\xspace}
\newcommand{\sem}[1]{[|#1|]}
\newcommand{\set}[1]{\{#1\}}
\newcommand{\incl}{\subseteq}
\renewcommand{\L}{\ensuremath{\mathcal{L}}\xspace}


%%% \squishlist definition, list with reduced margins
\newcommand{\squishlist}{
 \begin{list}{$\bullet$}
  { \setlength{\itemsep}{0pt}
     \setlength{\parsep}{3pt}
     \setlength{\topsep}{3pt}
     \setlength{\partopsep}{0pt}
     \setlength{\leftmargin}{1em}
     \setlength{\labelwidth}{1em}
     \setlength{\labelsep}{0.5em} } }

\newcommand{\squishlisttwo}{
 \begin{list}{$\bullet$}
  { \setlength{\itemsep}{0pt}
    \setlength{\parsep}{0pt}
    \setlength{	opsep}{0pt}
    \setlength{\partopsep}{0pt}
    \setlength{\leftmargin}{2em}
    \setlength{\labelwidth}{1.5em}
    \setlength{\labelsep}{0.5em} } }

\newcommand{\squishend}{
  \end{list}  }


\begin{document}
%
\frontmatter          
%
\pagestyle{headings}  

\mainmatter              
%
\title{
Rule Learning from Knowledge Graphs\\ Guided by Embedding Models\\ (Technical Report)}
\titlerunning{}  

\newcommand{\authspace}{\hspace{1.5ex}}
\author{%
  % ~
Vinh Thinh Ho$^{1}$,
Daria Stepanova$^{1}$,
Mohamed H. Gad-Elrab$^{1}$,\\
Evgeny Kharlamov$^{2}$,
Gerhard Weikum$^{1}$
}		
\institute{
%1
	$^1$Max Planck Institute for Informatics, Saarbr\"ucken, Germany\\
    $^2$University of Oxford, Oxford, United Kingdom
%3
}


\maketitle        


\begin{abstract}

Rules over a Knowledge Graph (KG) capture 
interpretable 
patterns in data
and various methods for rule learning have been proposed. 
Since KGs are inherently incomplete,
rules can be used to deduce missing facts. 
Statistical measures for learned rules such as confidence reflect rule quality well when the KG
is reasonably complete; however, these measures might be misleading otherwise. 
So it is difficult to learn high-quality
rules from the KG alone, 
and scalability dictates that only a small set
of candidate rules could be generated.
Therefore, the ranking and pruning 
of candidate rules are major problems.
To address this issue, we propose a rule learning method that utilizes probabilistic representations of missing facts. In particular, we iteratively extend rules induced from a KG by relying on feedback from a precomputed embedding model over the KG and
external information sources including text corpora. Experiments on real-world KGs demonstrate the effectiveness of our novel approach both with respect to the quality of the learned rules and fact predictions that they produce.
\end{abstract}

\input{sections/10-intro}
\input{sections/30-problem_statement.tex}
\input{sections/40-methodology.tex}
\input{sections/50-evaluation.tex}

\input{sections/60-related_work.tex}
\input{sections/70-conclusion.tex}
\input{sections/80-appendix.tex}

\bibliographystyle{abbrv}
\bibliography{references}



\end{document}
