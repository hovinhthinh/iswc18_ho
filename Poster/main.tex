\documentclass{llncs}
%
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{makeidx}  % allows for indexgeneration
%
\usepackage{paralist} 
\usepackage{epsfig} 
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{color}
\usepackage{multirow}
\usepackage{subfig}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    %linkcolor=blue,
%    filecolor=magenta,      
    urlcolor=blue,
    citecolor=blue,
    linkcolor=blue,
}

\usepackage[inline]{enumitem}
\input{commands}


\usepackage{llncs-space}

\newcommand{\G}{\ensuremath{\mathcal{G}}\xspace}
\newcommand{\X}{\ensuremath{\mathcal{X}}\xspace}
\newcommand{\C}{\ensuremath{\mathcal{C}}\xspace}
\newcommand{\R}{\ensuremath{\mathcal{R}}\xspace}
\newcommand{\PG}{\ensuremath{\mathcal{P}}\xspace}
\newcommand{\PW}{\ensuremath{\mathsf{PW}}\xspace}
\newcommand{\sem}[1]{[|#1|]}
\newcommand{\set}[1]{\{#1\}}
\newcommand{\incl}{\subseteq}
\renewcommand{\L}{\ensuremath{\mathcal{L}}\xspace}


%%% \squishlist definition, list with reduced margins
\newcommand{\squishlist}{
 \begin{list}{$\bullet$}
  { \setlength{\itemsep}{0pt}
     \setlength{\parsep}{3pt}
     \setlength{\topsep}{3pt}
     \setlength{\partopsep}{0pt}
     \setlength{\leftmargin}{1em}
     \setlength{\labelwidth}{1em}
     \setlength{\labelsep}{0.5em} } }

\newcommand{\squishlisttwo}{
 \begin{list}{$\bullet$}
  { \setlength{\itemsep}{0pt}
    \setlength{\parsep}{0pt}
    \setlength{	opsep}{0pt}
    \setlength{\partopsep}{0pt}
    \setlength{\leftmargin}{2em}
    \setlength{\labelwidth}{1.5em}
    \setlength{\labelsep}{0.5em} } }

\newcommand{\squishend}{
  \end{list}  }


\begin{document}
%
\frontmatter          % for the preliminaries
%
\pagestyle{headings}  % switches on printing of running heads
\addtocmark{Hamiltonian Mechanics} % additional mark in the TOC

\mainmatter              % start of the contributions
%
\title{
\hspace{-2ex}Learning Rules from Incomplete KGs using Embeddings%
\thanks{This poster is accompanying our conference paper~\cite{iswc-18}.}
}
%
\titlerunning{}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used

\newcommand{\authspace}{\hspace{1.5ex}}
\author{%
  % ~
Vinh Thinh Ho$^{1}$,
Daria Stepanova$^{1}$,
Mohamed H. Gad-Elrab$^{1}$,\\
Evgeny Kharlamov$^{2}$,
Gerhard Weikum$^{1}$
}		
\institute{
%1
	$^1$Max Planck Institute for Informatics, Saarland Informatics Campus, %Saarbr\"ucken, 
	Germany\\
    $^2$University of Oxford, Oxford, United Kingdom
%3
}

\maketitle              % typeset the title of the contribution


\begin{abstract}

Rules over a Knowledge Graph (KG) capture interpretable patterns in data and various methods for rule learning have been proposed. Since KGs are inherently incomplete, rules can be used to deduce missing facts. Statistical measures for learned rules such as confidence reflect rule quality well when the KG is reasonably complete; however, these measures might be misleading otherwise. So it is difficult to learn high-quality rules from the KG alone, and scalability dictates that only a small set of candidate rules is generated. Therefore, the ranking and pruning of candidate rules is a major problem. To address this issue, we propose a rule learning method that utilizes probabilistic representations of missing facts. In particular, we iteratively extend rules induced from a KG by relying on feedback from a precomputed embedding model over the KG and external information sources including text corpora. Experiments on real-world KGs demonstrate the effectiveness of our novel approach both with respect to the quality of the learned rules and fact predictions that they produce.
\end{abstract}


\subsubsection*{Motivation.}
Rules are widely used to 
represent relationships and dependencies among %between
 data items in datasets
and to capture the underlying patterns in data%
~\cite{Agrawal:1993:MAR:170036.170072,DBLP:books/mit/PF91/Piatetsky91}.
Applications of rules 
include health-care~\cite{DBLP:series/isrl/Wojtusiak14}, telecommunications~\cite{DBLP:conf/kdd/MannilaTV95}, 
manufacturing~\cite{DBLP:journals/ki/AuriolMG96,DBLP:conf/semweb/MehdiKSXKBHRR17a,DBLP:conf/semweb/MehdiKSXKBHRR17,DBLP:conf/cikm/MehdiKSXKBHRR17,DBLP:conf/cikm/KharlamovSXPMRH17}, and
commerce~\cite{DBLP:conf/pkdd/RasW00,DBLP:journals/kais/ImRW10}.
%
In order to facilitate rule construction, a variety of
rule learning methods have been developed,
see e.g.~\cite{association-rule-mining-overview} for an overview.
Moreover, various statistical measures such as confidence,
actionability, and unexpectedness to evaluate the \emph{quality} of %learner 
the learned rules have been proposed. 

Rule learning has recently been adapted %to
for the setting of
Knowledge Graphs (KGs)~\cite{amie,DBLP:journals/corr/WangL15i}
where data is represented as a graph of entities interconnected via relations and labeled with classes, or more formally as a set of grounded binary and unary atoms typically referred to as facts.
Examples of large-scale KGs include 
Wikidata~\cite{wikidata}, 
%NELL~\cite{NELL-aaai15}, 
Yago~\cite{yago},
 and Google's KG.
Since many KGs are constructed from semi-structured knowledge, such
as Wikipedia, or harvested from the Web with a combination of statistical and linguistic methods, they are inherently incomplete~\cite{DBLP:journals/semweb/Paulheim17,amie}. 

Rules over KGs are of the form $\mi{head \leftarrow body}$,
where $\mi{head}$ is a binary atom and 
$\mi{body}$ is a conjunction of, possibly negated, binary or unary atoms.
When rules are automatically learned, statistical measures like
support and confidence are used to assess the quality of rules.
Most notably, the confidence of a rule is the fraction of
facts predicted by the rule that are indeed true in the KG.
However, this is a meaningful measure for rule quality only
when the KG is reasonably complete.
For rules learned from largely incomplete KGs, confidence and
other measures may be misleading, as they do not reflect
the patterns in the missing facts.
For example, a KG that knows only (or mostly) male CEOs
would yield a heavily biased rule 
$\mi{gender(X,male) \leftarrow isCEO(X,Y), isCompany(Y)}$, 
which does not
extend to the entirety of valid facts beyond the KG.
Therefore, it is crucial that rules can be ranked by a meaningful
quality measure, % of quality, 
which accounts for %takes 
KG incompleteness. %into consideration.


%
\subsubsection{Example.}
Consider a KG about people's jobs, residence and
spouses as well as office locations and headquarters of companies.
Suppose a rule learning method has computed the following two rules:
{\small
\begin{align}
	\label{eq:ex-rule-1}
\mi{r_1 :\ }& \mi{livesIn(X,Y) \leftarrow worksFor(X,Z), hasOfficeIn(Z,Y)}\\[-.5ex]
	\label{eq:ex-rule-2}
\mi{r_2 :\ }& \mi{ livesIn(Y,Z) \leftarrow marriedTo(X,Y), livesIn(X,Z)}%\\[-.5ex]
\end{align}}
The rule $\mi{r_1}$ is quite noisy, as companies have offices in many cities, %and
but employees live and work in only one of them, while
the rule $\mi{r_2}$ clearly is of higher quality. 
However, depending on how the KG is populated with instances,
the rule $\mi{r_1}$ could nevertheless score higher than $\mi{r_2}$ in terms of confidence
measures. 
For example, the KG may contain only a specific subset of company offices
and only people who work for specific companies. 
%However, i
If we knew the complete KG, then the rule $\mi{r_2}$
should presumably be ranked higher than $\mi{r_1}$.

Suppose we had a perfect oracle for the true and complete KG. Then
we could learn even more sophisticated rules such as
 {\small
 \begin{align*}
\mi{r_3 :\ } &\mi{livesIn(X,Y) \leftarrow worksFor(X,Z), hasHeadquarterIn(Z,Y), \mi{not}\, locatedIn(Y,USA)}.%[-1.5ex]
\end{align*}}
This rule would capture that most people work in the same city
as their employers' headquarters, with the USA being an exception
(assuming that people there are used to long commutes).
This is an example of a rule that contains a negated atom in
the rule body (so it is no longer a Horn rule) and has a partially
grounded atom with a variable and a constant as its arguments.


\subsubsection{Problem.}
The problem of  KG incompleteness has been tackled by methods that
(learn to) predict missing facts for KGs 
(or actually missing relational edges between
existing entities).
A prominent class of approaches is statistics-based and includes
tensor factorization, e.g.,~\cite{conf/icml/NickelTK11} and neural-embedding-based models, e.g.~\cite{Bordes:NIPS2013,DBLP:conf/aaai/NickelRP16}.
Intuitively, these approaches turn a KG, possibly augmented with
external sources such as 
text% corpora
~\cite{DBLP:conf/ijcai/WangL16,DBLP:conf/aaai/0005HMZ17,DBLP:conf/esws/RingsquandlK0HL18}, 
into a probabilistic representation
of its entities and relations,
known as \emph{embeddings}, 
and then predict the likelihood of %a 
missing facts by reasoning over the embeddings~\cite{DBLP:journals/tkde/WangMWG17}.


These kinds of embeddings can complement the given KG and are a potential asset
in overcoming the limitations that arise from incomplete KGs.
Consider the following gedankenexperiment:
we compute embeddings from the KG and external text sources,
that can then be used to predict the complete KG that comprises all valid facts.
This would seemingly be the perfect starting point for learning rules,
without the bias and quality problems of the incomplete KG.
However, this scenario is way oversimplified.
The embeddings-based fact predictions would themselves be very noisy,
yielding also many spurious facts.
Moreover, the computation of all fact predictions and the induction of all
possible rules would come with a big scalability challenge: in practice,
we need to restrict ourselves to computing merely 
small subsets of likely fact predictions
and promising rule candidates.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Our Approach.}
In this work we propose an approach for rule learning guided by external sources
that allows to learn high-quality rules from incomplete KGs.
In particular, our method extends rule learning
by exploiting probabilistic representations of missing facts 
computed by embedding models of KGs and possibly other external information sources. 

More formally, 
let $\G$ be a KG, then a \emph{probabilistic KG} $\PG$ is a pair $\PG = (\G,f)$ 
where $f$ is a probability function over the facts (or KGs edges), where we assume $f(a) = 1$ for each fact $a \in \G$, which is already known to be true.
Our proposal is to learn rules that not only describe the available graph $\cG$ well, but also predict highly probable facts based on the function $f$.
The key questions here are how to define the quality of a given rule $r$ based on $\PG$ and how to exploit it during rule learning for pruning out not promising rules. 
We define a quality measure $\mu$ for rules over probabilistic KGs 
as a function $\mu: (r,\PG) \mapsto \alpha$, where $\alpha \in  [0,1]$.
% We propose t
To measure the quality $\mu$ of $r$ over $\PG$ we propose: 
\begin{compactitem}
	\item 
to measure the quality $\mu_1$ of $r$ over $\G$, where $\mu_1: (r,\cG) \mapsto \alpha \in  [0,1]$,
\item
to measure the quality $\mu_2$ of $\cG_r$ (i.e., $\cG$ extended with edges derived using $r$) by relying on $\PG_r=(\cG_r,f)$, where $\mu_2{:}\, (\G'{,}\, (\G,f)) \mapsto  \alpha \,{\in}\, [0,1]$ for $\G' \supseteq \G$ is the quality of extensions $\G'$ of $\G$ over the signature of $\G$ given $f$,
and 
\item
to combine the result as the weighted sum.
\end{compactitem}
That is, we define our hybrid rule quality function $\mu(r,\PG)$ as follows:
\begin{align}\label{eq:hm}
	\mu(r,\PG)= (1 - \lambda)\times \mu_1(r,\cG) + \lambda \times \mu_2(\G_r,\PG)
\end{align}
In this formula $\mu_1$ can be any classical quality measure of rules over complete graphs. %  and 
% $\mu_2$ 
% \thi{mu2 what?}.
Intuitively, $\mu_2(\G_r,\PG)$ is the quality of $\G_r$ wrt $f$ that allows us to capture the information about facts missing in $\G$ that are relevant for $r$.
The weighting factor %parameter 
$\lambda$, we call it \textit{embedding weight}, allows one to choose whether %one prefers 
to rely more on the classical measure $\mu_1$ or on the measure $\mu_2$ of 
%the quality of the facts that are predicted by $r$ over $\G$.
the quality of the extension $\G_r$. %of $r$ over $\G$.

We propose to realize this approach by iteratively constructing rules over a KG 
and by collecting feedback from a precomputed embedding model, 
through specific queries issued to the model 
for assessing the quality of (partially constructed) rule candidates. 
This way, the rule induction loop is interleaved with the guidance from the embeddings,
and we avoid scalability problems.
Our machinery is also more expressive than many prior works on rule learning from KGs,
by allowing non-monotonic rules with negated atoms as well as partially grounded atoms. 
Within this framework, we devise confidence measures that capture rule quality
better than previous techniques and thus improve the ranking of rules.

\subsubsection{Contribution.}
We propose a rule learning approach  guided by external sources,
	 and show how to learn high-quality rules by utilizing feedback from embedding models. 
We implement our approach and present extensive experiments on real-world KGs,
demonstrating the effectiveness of our approach with respect to both %\wrt\ 
the quality of the mined rules and predictions that they produce. 
Our code and data %is 
are made available to the research community at
\url{https://github.com/hovinhthinh/RuLES}.




\vspace{-.2cm}
\bibliographystyle{abbrv}
\bibliography{references}


\end{document}

